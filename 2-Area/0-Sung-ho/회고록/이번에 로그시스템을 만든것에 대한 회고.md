---
created: 2024-03-02T22:41:32
date: 2024-03-03T11:41
updated: 2024-03-31T22:43
---
### 구조

[![structure](https://user-images.githubusercontent.com/58648988/259431652-79424eda-d632-47e7-9f1e-a981c288b49f.png)](https://user-images.githubusercontent.com/58648988/259431652-79424eda-d632-47e7-9f1e-a981c288b49f.png)

### library

라이브러리를 만들면서 첫번째로한 고민은 이미 구현되어있는 라이브러리(logback)을 사용할것인가, 아니면 직접 구현할것인가 였다.  
직접구현의 장점은.. 1. 재미가 있을것 같다, 2. 정말 필요한 기능만 구현하기에 구현되어있는 라이브러리보다 가볍다.  
라이브러리의 장점은.. 1. 편하다, 2. 기능의 구현이 보장되어있다. 3. 안정적이다..  
시간도 많지 않았고 안정성을 위해 라이브러리를 사용하여 구현하였다.

logging을 위한 라이브러리는 logback을 사용하였다.  
logback은 대부분 xml로 설정하지만(설정 확인하고 수정하기가 쉬우니) maven에 publishing해서 사용예정이라 programmatic하게 구현하였다.

logback 사용시 주의사항

- rolling시 pattern으로 rolling이 된다
- isAdditive 해당 로거에서 발생한 이벤트를 부모에게 전달할지 여부를 결정

로그를 쌓을 때 큰 문제가 없다면 로그는 비동기로 쌓자

- 로그쌓는거 자체가 로직에 부담을 주면 안되기 때문!

logging-lib는 간단한 설정을 bean 주입(프로젝트명, 환경)받아 logger bean을 등록하는 방법으로 만들었다.

### efs

efs는 elastic한 network file storage이다.  
efs에 저장된 데이터를 local로 가져오는 부분에서 시간을 좀 잡아먹었다.  
처음에는 efs자체를 로컬 서버에 mount하려 했었다.  
로컬 서버에 efs를 mount하려면 vpn을 사용해야 하는데 기존에 사용하고 있던 vpn이 없었고, vpn구축하고 24시간동안 연결하는 비용보다, ec2인스턴스에 filebeat를 올리고 filebeat에서 로컬서버의 logstashfh 전송하는 방법을 택했다.  
추가로 vpn설정에서 문제가 생겼을때 문제 해결이 쉽지 않았을 것도 고려되었었다.

filebeat에서 logstash로 보내는 [@timestamp](https://github.com/timestamp)는 filebeat에서 로그를 읽은 시간임

- filebeat가 도중에 중지되었다면 중지된 기간동안 로그의 [@timestamp](https://github.com/timestamp)는 다시 시작한 시간이후로 설정됨
- 나는 로그에도 따로 timestamp를 넣었음

결과적으로 각 어플리케이션에서 efs를 마운트하여 efs에 로그를 쌓고, ec2에서 filebeat를 실행하여 로컬서버로 로그를 전송한다.

### elk

aws opensearch를 통해 elasticsearch에 데이터를 저장 후 , kibana, elasticsearch에서 조회만 해봤지 logastash를 만져본적은 처음이었고, elk스택 서버를 올리는것도 처음이었다.  
elk는 docker를 사용하여 실행하였다.  
zgc를 사용해보려 했지만 elastic search는 zgc에서 충분히 테스트 되지 않았고, 지원하지 않아([링크](https://github.com/elastic/elasticsearch/issues/58989)참고) 기본 gc인 g1gc를 사용하였다.  
기본 세팅은 [https://github.com/deviantony/docker-elk기반으로](https://github.com/deviantony/docker-elk%EA%B8%B0%EB%B0%98%EC%9C%BC%EB%A1%9C) 작성하였습니다.  
logstash에서는 filebeat에서 받은 로그를 파싱해서 elasticsearch로 보냄  
logstash에서 로그를 파싱할때 플러그인을 사용함  
logstash filter plugin으로 prune, json, ruby를 사용했었는데 ruby가 짱이다  
좀 복잡한거는 ruby로 처리하는게 더 좋았음  
결과적으로 prune으로 필요없는 필드만 삭제함

추가로 heartbeat 호스트를 설정해야하는데 특정 호스트를 지정할 수 없다면 0.0.0.0을 적으면 된다는것도 깨달았다.